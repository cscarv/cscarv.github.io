---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
published: true
redirect_from:
  - /research
---

Some of the most exciting recent developments in machine learning exploit the *geometry* of objects like molecules, shapes, and social networks to

- [Discover new antibiotics](https://www.sciencedirect.com/science/article/pii/S0092867420301021)
- [Predict the physics of glass](https://www.nature.com/articles/s41567-020-0842-8)
- [Generate product recommendations at massive scale](https://www.amazon.science/publications/p-companion-a-principled-framework-for-diversified-complementary-product-recommendation)
- [Predict protein interaction sites](https://openaccess.thecvf.com/content/CVPR2021/papers/Sverrisson_Fast_End-to-End_Learning_on_Protein_Surfaces_CVPR_2021_paper.pdf)
- [Classify particle jets in the Large Hadron Collider](https://arxiv.org/abs/1902.08570)
- [Predict functional regions in the cerebral cortex](https://research.facebook.com/publications/convolutional-neural-networks-for-mesh-based-parcellation-of-the-cerebral-cortex/)
- [Predict molecular dynamics](https://www.nature.com/articles/s41467-022-29939-5)

But sometimes the default geometry of our data… isn’t right for the problem we’d like to solve.

Many real-world graphs have the small-world property. Most nodes are close to one another in these graphs, causing graph neural nets to exhibit pathological behavior like over-smoothing and over-squashing that make it difficult to transmit information between distant node pairs.

Assuming a Euclidean geometry for 
